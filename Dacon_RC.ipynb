{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNNg_sex39Dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73123e97-10aa-4b92-e445-761e05ba2045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbaFlR9-3vEN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from glob import glob\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer ,AutoModelForSequenceClassification,AutoConfig,Trainer,TrainingArguments , AutoModel\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0ZVLx4T3Dm7"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/gdrive/MyDrive/Data'\n",
        "train= pd.read_csv(os.path.join(data_path,'train_data.csv'))\n",
        "test = pd.read_csv(os.path.join(data_path,'test_data.csv'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "xyqc_sHl3LqF",
        "outputId": "469e1ef0-e09f-41a6-876b-125f8600703b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-70bd4ebd-b2d3-437e-8db5-e4db983e9e69\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
              "      <td>씨름의 여자들의 놀이이다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
              "      <td>자작극을 벌인 이는 3명이다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
              "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
              "      <td>원주민들은 종합대책에 만족했다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
              "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70bd4ebd-b2d3-437e-8db5-e4db983e9e69')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-70bd4ebd-b2d3-437e-8db5-e4db983e9e69 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-70bd4ebd-b2d3-437e-8db5-e4db983e9e69');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   index  ...          label\n",
              "0      0  ...  contradiction\n",
              "1      1  ...  contradiction\n",
              "2      2  ...     entailment\n",
              "3      3  ...        neutral\n",
              "4      4  ...        neutral\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd63oNwT4wNl",
        "outputId": "fc9151d2-c2b7-4022-ec6e-4072b74ef50c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실제 Text 평균 길이 : 45.406552524201935\n",
            "가설 Text 평균 길이 : 24.924433954716378\n",
            "최대 실제 텍스트 길이 :  90\n",
            "최대 가설 텍스트 길이 :  103\n"
          ]
        }
      ],
      "source": [
        "premise_len_mean = np.mean(train['premise'].str.len()) # 리뷰 길이의 평균값 계산\n",
        "print('실제 Text 평균 길이 :',premise_len_mean)\n",
        "\n",
        "hypo_len_mean = np.mean(train['hypothesis'].str.len()) # 리뷰 길이의 평균값 계산\n",
        "print('가설 Text 평균 길이 :',hypo_len_mean)\n",
        "\n",
        "max_prem_len = np.max(train['premise'].str.len())\n",
        "print('최대 실제 텍스트 길이 : ', max_prem_len)\n",
        "\n",
        "max_hypo_len = np.max(train['hypothesis'].str.len())\n",
        "print('최대 가설 텍스트 길이 : ', max_hypo_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZ7lhdSM5G4y"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 120\n",
        "batch_size = 128\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 30\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate = 1e-7\n",
        "model_name = \"Huffon/klue-roberta-base-nli\"\n",
        "device = torch.device(\"cuda:0\")\n",
        "## 베이스라인 설정. 최대 길이만 변경해뒀습니다"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class STModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = SentenceTransformer(\"ddobokki/klue-roberta-small-nli-sts\")\n",
        "        self.fc1 = nn.Linear(768,384)\n",
        "        self.fc2 = nn.Linear(384,192)\n",
        "        self.fc3 = nn.Linear(192,3)\n",
        "\n",
        "    def forward(self,input):\n",
        "        fc1_input = self.encoder(input)\n",
        "        fc2_input = self.fc1(fc1_input)\n",
        "        fc3_input = self.fc2(fc2_input)\n",
        "        output = self.fc3(fc3_input)\n",
        "        return output\n",
        "        "
      ],
      "metadata": {
        "id": "5T3wcX2qxOoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKkf4zUgk6fo"
      },
      "outputs": [],
      "source": [
        "class TorchDataset(Dataset):\n",
        "  def __init__(self,premise,hypothesis,labels,tokenizer):\n",
        "    premise = self.clean_text(premise)\n",
        "    hypothesis = self.clean_text(hypothesis)\n",
        "    self.len=len(premise)\n",
        "    label2idx={key:value for value,key in enumerate(self.get_label_list())}\n",
        "    self.labels=[label2idx[label] for label in labels]\n",
        "    self.encodings = tokenizer(premise,hypothesis, add_special_tokens = True , return_tensors = 'pt', padding = True, truncation = True, max_length = max_seq_len)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "    \n",
        "  def __getitem__(self,idx):\n",
        "    item = {key: val[idx] for key,val in self.encodings.items()} \n",
        "    item['labels'] = self.labels[idx]\n",
        "    return item\n",
        "\n",
        "  def clean_text(self,series):\n",
        "    return [self.clean(sent) for sent in series]\n",
        "\n",
        "  def clean(self,sent):\n",
        "    sent = re.sub('\\u00A0',' ',sent)\n",
        "    sent = re.sub(' +',' ',sent)\n",
        "    return sent\n",
        "\n",
        "  def get_data(self):\n",
        "    return self.labels\n",
        "  \n",
        "  def get_label_list(self):\n",
        "    return ['entailment','neutral','contradiction']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwlNuISwo3c0"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "train_dataset = TorchDataset(train['premise'],train['hypothesis'],train['label'],tokenizer)\n",
        "model_config = AutoConfig.from_pretrained(model_name)\n",
        "model_config.num_labels = 3\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name , config=model_config)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH1c_CbFgaFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b76315-67ad-4ffa-c3fd-baea2a8bd47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model preparation done\n"
          ]
        }
      ],
      "source": [
        "model.to('cuda:0')\n",
        "model.half()\n",
        "print(\"model preparation done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQLz-7LLD1Gw"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    pin_memory = (torch.cuda.is_available()),\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    num_workers = 4,\n",
        "    drop_last = True\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(),lr = learning_rate,momentum= 0.9)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer,\n",
        "                                                     T_max=10,\n",
        "                                                     eta_min=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "CppGXjL_14tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10relu6-9VQm",
        "outputId": "e6c6fb7c-44b5-4535-a0ce-4c824d73a243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 195/195 [03:46<00:00,  1.16s/it, Running accuracy=0.801, Running Loss=0.525]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.833, Running Loss=0.48]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.854, Running Loss=0.462]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.866, Running Loss=0.44]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.878, Running Loss=0.221]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.89, Running Loss=0.303]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.901, Running Loss=0.297]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.91, Running Loss=0.113]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.918, Running Loss=0.278]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.928, Running Loss=0.105]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.935, Running Loss=0.195]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.942, Running Loss=0.105]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.15s/it, Running accuracy=0.948, Running Loss=0.226]\n",
            "  0%|          | 0/195 [00:01<?, ?it/s]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.955, Running Loss=0.117]\n",
            "  0%|          | 0/195 [00:01<?, ?it/s]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.958, Running Loss=0.143]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.15s/it, Running accuracy=0.964, Running Loss=0.132]\n",
            "  0%|          | 0/195 [00:01<?, ?it/s]\n",
            "  0%|          | 0/195 [00:01<?, ?it/s]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.15s/it, Running accuracy=0.966, Running Loss=0.173]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.15s/it, Running accuracy=0.971, Running Loss=0.14]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.15s/it, Running accuracy=0.976, Running Loss=0.101]\n",
            "  0%|          | 0/195 [00:01<?, ?it/s]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.15s/it, Running accuracy=0.979, Running Loss=0.0313]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.15s/it, Running accuracy=0.977, Running Loss=0.0285]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.977, Running Loss=0.0943]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.981, Running Loss=0.0829]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.16s/it, Running accuracy=0.986, Running Loss=0.0866]\n",
            "100%|██████████| 195/195 [03:45<00:00,  1.15s/it, Running accuracy=0.986, Running Loss=0.0219]\n"
          ]
        }
      ],
      "source": [
        "def train_loop(model,num_epochs,train_dataloader):\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        run_count = 0\n",
        "        avg_loss = 0\n",
        "\n",
        "        running_loss, correct, total = 0.0, 0,0\n",
        "        preds, gt = [], []\n",
        "        pbar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
        "        model.train()\n",
        "\n",
        "        for batch, data in pbar:\n",
        "            \n",
        "            labels = data['labels'].to(\"cuda:0\")\n",
        "            data['attention_mask'] = data['attention_mask'].to(device)\n",
        "            data['input_ids'] = data['input_ids'].to(device)\n",
        "            data['token_type_ids'] = data['token_type_ids'].to(device)\n",
        "            outputs = model(input_ids = data['input_ids'],attention_mask = data['attention_mask']\n",
        "                            # ,token_type_ids = data['token_type_ids']                         \n",
        "                            )\n",
        "            \n",
        "            loss = criterion(outputs.logits, labels) \n",
        "            \n",
        "        \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            _, pred = torch.max(outputs.logits, 1)\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            correct += (pred == labels).sum().item()\n",
        "            preds += pred.to(\"cpu\").tolist()\n",
        "            gt += labels.to(\"cpu\").tolist()\n",
        "            if  correct/total >= 0.99:\n",
        "                break\n",
        "            pbar.set_postfix({\"Running accuracy\" : correct/total, \"Running Loss\" : loss.item()})\n",
        "            running_loss += loss.item()\n",
        "            run_count +=1\n",
        "            \n",
        "                \n",
        "        \n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = train_loop(model,num_epochs,train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciivC6VZ-Eb9"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset):\n",
        "  def __init__(self,premise,hypothesis,tokenizer):\n",
        "    premise = self.clean_text(premise)\n",
        "    hypothesis = self.clean_text(hypothesis)\n",
        "    self.len=len(premise)\n",
        "    label2idx={key:value for value,key in enumerate(self.get_label_list())}\n",
        "    self.encodings = tokenizer(premise,hypothesis, add_special_tokens = True , return_tensors = 'pt', padding = True, truncation = True, max_length = 120)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "    \n",
        "  def __getitem__(self,idx):\n",
        "    item = {key: val[idx] for key,val in self.encodings.items()}\n",
        "    return item\n",
        "\n",
        "  def clean_text(self,series):\n",
        "    return [self.clean(sent) for sent in series]\n",
        "\n",
        "  def clean(self,sent):\n",
        "    sent = re.sub('\\u00A0',' ',sent)\n",
        "    sent = re.sub(' +',' ',sent)\n",
        "    return sent\n",
        "  \n",
        "  def get_label_list(self):\n",
        "    return ['entailment','neutral','contradiction']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqaCFcrX5fgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf89ea5a-18ce-4bc4-8875-be16dd78118d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1666"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "test_dataset = TestDataset(test['premise'],test['hypothesis'],tokenizer)\n",
        "\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    pin_memory = (torch.cuda.is_available()),\n",
        "    shuffle=False,\n",
        "    batch_size=1,\n",
        "    num_workers = 8,\n",
        "    drop_last = False\n",
        ")\n",
        "len(test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFZLO2fS576g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "f346b489-b509-4a82-9db7-de82ecaf47d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "250it [00:07, 31.60it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-895db3455b20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# print(data['input_ids'][0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     outputs = model(input_ids = data['input_ids'],attention_mask = data['attention_mask']\n\u001b[0m\u001b[1;32m     15\u001b[0m                     \u001b[0;31m#,token_type_ids = data['token_type_ids']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1211\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         )\n\u001b[1;32m   1215\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    531\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m                 )\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         )\n\u001b[1;32m    419\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.90 GiB total capacity; 14.08 GiB already allocated; 21.75 MiB free; 14.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "predictions = []\n",
        "logits_savings = []\n",
        "count = 0\n",
        "\n",
        "\n",
        "submission = pd.read_csv(os.path.join(data_path,'sample_submission.csv'))\n",
        "\n",
        "idx2label = {0:'entailment',1:'neutral',2:'contradiction'}\n",
        "for idx,data in tqdm(enumerate(test_dataloader)):\n",
        "    data['attention_mask'] = data['attention_mask'].to(device)\n",
        "    data['input_ids'] = data['input_ids'].to(device)\n",
        "    data['token_type_ids'] = data['token_type_ids'].to(device)\n",
        "    # print(data['input_ids'][0])\n",
        "    outputs = model(input_ids = data['input_ids'],attention_mask = data['attention_mask']\n",
        "                    #,token_type_ids = data['token_type_ids']\n",
        "                    )\n",
        "    \n",
        "    #pred = torch.argmax(outputs.logits,dim=1)\n",
        "    logits_savings.append(outputs.logits)\n",
        "    #pred = [idx2label[i.item()] for i in pred]\n",
        "    #predictions+=pred\n",
        "    \n",
        "print(predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwEtmKIy_aNn"
      },
      "outputs": [],
      "source": [
        "submission[\"label\"] = predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mIIRXZxONgM"
      },
      "outputs": [],
      "source": [
        "submission.to_csv(\"/content/gdrive/MyDrive/Data/submissions/New_Model_30epoch.csv\", index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x5idqnYOOs8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e82c1c0-5ef6-40a5-a26a-7717dbe7378e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'contradiction': 520, 'entailment': 491, 'neutral': 655})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from collections import Counter\n",
        "Counter(submission['label'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jp4RXRjlT7Zn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Dacon_RC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}